"""
Advanced Unified Content Search - ××¢×¨×›×ª ×—×™×¤×•×© ××ª×§×“××ª ×œ×ª×•×›×Ÿ ×××•×—×“
××‘×•×¡×¡×ª ×¢×œ ×”××™× ×“×§×¡ ×”×××•×—×“ ×©× ×•×¦×¨ ×¢× unified_indexer.py
×ª×•××›×ª ×‘×—×™×¤×•×© ×‘×•×™×“××• ×•××¡××›×™× ×™×—×“ ××• ×‘× ×¤×¨×“
"""
import logging
from typing import List, Dict
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery
from openai import AzureOpenAI
import traceback
from Config.config import (
    SEARCH_SERVICE_NAME, SEARCH_API_KEY,
    AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_VERSION,
    AZURE_OPENAI_EMBEDDING_MODEL, INDEX_NAME
)
from Config.logging_config import setup_logging

# Initialize logger
logger = setup_logging()


class AdvancedUnifiedContentSearch:
    """
    ××¢×¨×›×ª ×—×™×¤×•×© ××ª×§×“××ª ×œ×ª×•×›×Ÿ ×××•×—×“ - ×•×™×“××• ×•××¡××›×™×
    ×ª×•××›×ª ×‘×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™, ×¡×× ×˜×™ ×•×•×§×˜×•×¨×™
    ×××¤×©×¨×ª ×—×™×¤×•×© ×‘×›×œ ×”×ª×•×›×Ÿ ×™×—×“ ××• ×‘×¡×™× ×•×Ÿ ×œ×¤×™ ×¡×•×’
    """

    def __init__(self, index_name: str = INDEX_NAME):
        self.index_name = INDEX_NAME
        self.search_endpoint = f"https://{SEARCH_SERVICE_NAME}.search.windows.net"
        self.credential = AzureKeyCredential(SEARCH_API_KEY)

        # ×™×¦×™×¨×ª search client
        self.search_client = SearchClient(
            endpoint=self.search_endpoint,
            index_name=self.index_name,
            credential=self.credential
        )

        # ×™×¦×™×¨×ª OpenAI client ×œ×—×™×¤×•×© ×•×§×˜×•×¨×™
        self.openai_client = AzureOpenAI(
            api_key=AZURE_OPENAI_API_KEY,
            api_version=AZURE_OPENAI_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT
        )

        logger.info(f"AdvancedUnifiedContentSearch initialized with index: {self.index_name}")

    def check_index_status(self) -> Dict:
        """×‘×“×™×§×ª ××¦×‘ ×”××™× ×“×§×¡ ×”×××•×—×“ ×•×”×¦×’×ª ××™×“×¢ ×‘×¡×™×¡×™"""
        logger.info("=" * 60)

        try:
            # ×—×™×¤×•×© ×›×œ×œ×™ ×œ×‘×“×™×§×”
            results = self.search_client.search(
                search_text="*",
                select=["*"],
                top=5,
                include_total_count=True
            )

            total_count = results.get_count()
            docs = list(results)

            logger.info(f"ğŸ“Š ×¡×”\"×› ×¦'×× ×§×™× ×‘××™× ×“×§×¡ ×”×××•×—×“: {total_count}")
            logger.info(f"ğŸ“„ ××¡××›×™× ×©×”×•×—×–×¨×• ×œ×‘×“×™×§×”: {len(docs)}")

            if docs:
                logger.info(f"âœ… ×”××™× ×“×§×¡ ×”×××•×—×“ ×¤×¢×™×œ ×•××›×™×œ × ×ª×•× ×™×")

                # ×¡×¤×™×¨×” ×œ×¤×™ ×¡×•×’ ×ª×•×›×Ÿ
                video_results = self.search_client.search("*", filter="content_type eq 'video'",
                                                          include_total_count=True, top=0)
                video_count = video_results.get_count()

                doc_results = self.search_client.search("*", filter="content_type eq 'document'",
                                                        include_total_count=True, top=0)
                doc_count = doc_results.get_count()

                logger.info(f"ğŸ¥ ×•×™×“××• ×¦'×× ×§×™×: {video_count}")
                logger.info(f"ğŸ“ ××¡××š ×¦'×× ×§×™×: {doc_count}")

                # ×”×¦×’×ª ×“×•×’×××•×ª ×œ××¡××›×™×
                logger.info(f"\nğŸ“„ ×“×•×’×××•×ª ×œ××¡××›×™× ×‘××™× ×“×§×¡:")
                for i, doc in enumerate(docs[:10], 1):
                    content_type = doc.get('content_type', 'unknown')
                    logger.info(f"\nğŸ“„ ××¡××š {i} ({content_type}):")
                    logger.info(f"  ğŸ†” ID: {doc.get('id', 'N/A')}")
                    logger.info(f"  ğŸ“„ Source ID: {doc.get('source_id', 'N/A')}")
                    logger.info(f"  ğŸ“ Source Name: {doc.get('source_name', 'N/A')}")
                    logger.info(f"  ğŸ“‘ Chunk Index: {doc.get('chunk_index', 'N/A')}")

                    if content_type == 'video':
                        logger.info(f"  â° Start Time: {doc.get('start_time', 'N/A')}")
                        logger.info(f"  â±ï¸ Start Seconds: {doc.get('start_seconds', 'N/A')}")
                    elif content_type == 'document':
                        logger.info(f"  ğŸ“‹ Section Title: {doc.get('section_title', 'N/A')}")
                        logger.info(f"  ğŸ“„ Document Type: {doc.get('document_type', 'N/A')}")

                    # ×”×¦×’×ª ×ª×•×›×Ÿ ×”×˜×§×¡×˜
                    text = doc.get('text', '')
                    if text:
                        preview = text[:150] + "..." if len(text) > 150 else text
                        logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")
                    logger.info("-" * 30)

                return {
                    "status": "active",
                    "total_chunks": total_count,
                    "video_chunks": video_count,
                    "document_chunks": doc_count,
                    "sample_doc": docs[0] if docs else None
                }
            else:
                logger.info("âš ï¸ ×”××™× ×“×§×¡ ×§×™×™× ××‘×œ ×¨×™×§")
                return {"status": "empty", "total_chunks": 0}

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×’×™×©×” ×œ××™× ×“×§×¡: {e}")
            logger.error(f"Error checking index status: {e}")
            return {"status": "error", "error": str(e)}

    def generate_query_embedding(self, query: str) -> List[float]:
        """×™×¦×™×¨×ª embedding ×œ×©××œ×ª ×”×—×™×¤×•×©"""
        try:
            response = self.openai_client.embeddings.create(
                model=AZURE_OPENAI_EMBEDDING_MODEL,
                input=query
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating query embedding: {e}")
            return []

    def simple_text_search(self, query: str, top_k: int = 5, source_id: str = None, course_id: str = None) -> List[
        Dict]:
        """×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×¤×©×•×˜ ×‘××™×“×” ×•×œ× × ×™×ª×Ÿ ×œ×—×œ×¥ embedding"""
        logger.info("=" * 60)

        try:
            search_params = {
                "search_text": query,
                "select": [
                    "id", "content_type", "source_id", "course_id", "chunk_index",
                    "text", "start_time", "end_time", "section_title", "created_date", "keywords", "topics"
                ],
                "top": top_k,
                "include_total_count": True
            }

            # ×”×•×¡×¤×ª ×¤×™×œ×˜×¨×™×
            filters = []
            if source_id:
                escaped_source_id = source_id.replace("'", "''")
                filters.append(f"source_id eq '{escaped_source_id}'")
            if course_id:
                escaped_course_id = course_id.replace("'", "''")
                filters.append(f"course_id eq '{escaped_course_id}'")

            if filters:
                search_params["filter"] = " and ".join(filters)

            results = self.search_client.search(**search_params)

            docs = list(results)
            total_count = results.get_count()

            if not docs:
                logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª")
                return []

            filter_msg = self._build_filter_message(source_id, course_id)
            logger.info(f"âœ… × ××¦××• {len(docs)} ×ª×•×¦××•×ª ××ª×•×š {total_count} ×¦'×× ×§×™×{filter_msg}:")

            for i, doc in enumerate(docs, 1):
                score = doc.get('@search.score', 0)
                content_type_doc = doc.get('content_type', 'unknown')
                logger.info(f"\nğŸ“„ ×ª×•×¦××” {i} ({content_type_doc}, ×¦×™×•×Ÿ: {score:.3f}):")
                logger.info(f"  ğŸ†” ID: {doc.get('id', 'N/A')}")
                logger.info(f"  ğŸ“„ ××§×•×¨ ID: {doc.get('source_id', 'N/A')}")
                logger.info(f"  ğŸ“š ×§×•×¨×¡ ID: {doc.get('course_id', 'N/A')}")
                logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")
                logger.info(f"  ğŸ“… ×ª××¨×™×š ×™×¦×™×¨×”: {doc.get('created_date', 'N/A')}")

                if content_type_doc == 'video':
                    start_time = doc.get('start_time', '')
                    end_time = doc.get('end_time', '')
                    if start_time:
                        logger.info(f"  â° ×–××Ÿ: {start_time} - {end_time}")
                    keywords = doc.get('keywords', '')
                    if keywords:
                        logger.info(f"  ğŸ” ××™×œ×•×ª ××¤×ª×—: {keywords}")
                    topics = doc.get('topics', '')
                    if topics:
                        logger.info(f"  ğŸ·ï¸ × ×•×©××™×: {topics}")
                elif content_type_doc == 'document':
                    section_title = doc.get('section_title', '')
                    if section_title:
                        logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª ×¡×¢×™×£: {section_title}")

                text = doc.get('text', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")

                logger.info("â€”" * 40)

            return docs

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™: {e}")
            logger.error(f"Error in text search: {e}")
            return []

    def hybrid_search(self, query: str, top_k: int = 5, source_id: str = None, course_id: str = None) -> List[Dict]:
        """×—×™×¤×•×© ×”×™×‘×¨×™×“×™ - ××©×œ×‘ ×˜×§×¡×˜ ×•×•×§×˜×•×¨"""
        logger.info("=" * 60)

        try:
            # ×™×¦×™×¨×ª embedding ×œ×©××œ×”
            query_vector = self.generate_query_embedding(query)
            if not query_vector:
                logger.info("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×™×¦×•×¨ embedding, ××‘×¦×¢ ×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×‘×œ×‘×“")
                return self.simple_text_search(query, top_k, source_id, course_id)

            search_params = {
                "search_text": query,
                "vector_queries": [VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=50,
                    fields="vector"
                )],
                "select": [
                    "id", "content_type", "source_id", "course_id", "chunk_index",
                    "text", "start_time", "end_time", "section_title", "created_date", "keywords", "topics"
                ],
                "top": 50,
                "include_total_count": True
            }

            # ×”×•×¡×¤×ª ×¤×™×œ×˜×¨×™×
            filters = []
            if source_id:
                escaped_source_id = source_id.replace("'", "''")
                filters.append(f"source_id eq '{escaped_source_id}'")
            if course_id:
                escaped_course_id = course_id.replace("'", "''")
                filters.append(f"course_id eq '{escaped_course_id}'")

            if filters:
                search_params["filter"] = " and ".join(filters)

            results = self.search_client.search(**search_params)

            docs = list(results)
            total_count = results.get_count()

            if not docs:
                logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª ×”×™×‘×¨×™×“×™×•×ª")
                return []

            # Slice to requested top_k for display and return
            docs = docs[:top_k]

            filter_msg = self._build_filter_message(source_id, course_id)
            logger.info(f"âœ… × ××¦××• {len(docs)} ×ª×•×¦××•×ª ×”×™×‘×¨×™×“×™×•×ª ××ª×•×š {total_count} ×¦'×× ×§×™×{filter_msg}:")

            for i, doc in enumerate(docs, 1):
                score = doc.get('@search.score', 0)
                content_type_doc = doc.get('content_type', 'unknown')
                logger.info(f"\nğŸ“„ ×ª×•×¦××” {i} ({content_type_doc}, ×¦×™×•×Ÿ ××©×•×œ×‘: {score:.3f}):")
                logger.info(f"  ğŸ†” ID: {doc.get('id', 'N/A')}")
                logger.info(f"  ğŸ“„ ××§×•×¨ ID: {doc.get('source_id', 'N/A')}")
                logger.info(f"  ğŸ“š ×§×•×¨×¡ ID: {doc.get('course_id', 'N/A')}")
                logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")
                logger.info(f"  ğŸ“… ×ª××¨×™×š ×™×¦×™×¨×”: {doc.get('created_date', 'N/A')}")

                if content_type_doc == 'video':
                    start_time = doc.get('start_time', '')
                    end_time = doc.get('end_time', '')
                    if start_time:
                        logger.info(f"  â° ×–××Ÿ: {start_time} - {end_time}")
                    keywords = doc.get('keywords', '')
                    if keywords:
                        logger.info(f"  ğŸ” ××™×œ×•×ª ××¤×ª×—: {keywords}")
                    topics = doc.get('topics', '')
                    if topics:
                        logger.info(f"  ğŸ·ï¸ × ×•×©××™×: {topics}")
                elif content_type_doc == 'document':
                    section_title = doc.get('section_title', '')
                    if section_title:
                        logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª ×¡×¢×™×£: {section_title}")

                text = doc.get('text', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")

                logger.info("â€”" * 40)

            return docs

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ×”×™×‘×¨×™×“×™: {e}")
            logger.error(f"Error in hybrid search: {e}")
            return []

    def semantic_search(self, query: str, top_k: int = 5, source_id: str = None, course_id: str = None) -> List[
        Dict]:
        """×—×™×¤×•×© ×¡×× ×˜×™ ××ª×§×“×"""
        logger.info("=" * 60)

        try:
            # ×™×¦×™×¨×ª embedding ×œ×©××œ×”
            query_vector = self.generate_query_embedding(query)
            if not query_vector:
                logger.info("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×™×¦×•×¨ embedding, ××‘×¦×¢ ×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×‘×œ×‘×“")
                return self.simple_text_search(query, top_k, source_id, course_id)

            # ×”×›× ×ª ×¤×¨××˜×¨×™× ×œ×—×™×¤×•×©
            search_params = {
                "search_text": query,
                "query_type": "semantic",
                "semantic_configuration_name": "default",
                "query_language": "he-il",
                "highlight_fields": "text",
                "vector_queries": [VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=top_k,
                    fields="vector"
                )],
                "select": [
                    "id", "content_type", "source_id", "course_id", "chunk_index",
                    "text", "start_time", "end_time", "section_title", "created_date", "keywords", "topics"
                ],
                "top": top_k
            }

            # ×”×•×¡×¤×ª ×¤×™×œ×˜×¨×™×
            filters = []
            if source_id:
                escaped_source_id = source_id.replace("'", "''")
                filters.append(f"source_id eq '{escaped_source_id}'")
            if course_id:
                escaped_course_id = course_id.replace("'", "''")
                filters.append(f"course_id eq '{escaped_course_id}'")

            if filters:
                search_params["filter"] = " and ".join(filters)

            # ×—×™×¤×•×© ×¡×× ×˜×™ ××ª×§×“×
            results = self.search_client.search(**search_params)

            docs = list(results)

            if not docs:
                logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª ×¡×× ×˜×™×•×ª")
                return []

            filter_msg = self._build_filter_message(source_id, course_id)
            logger.info(f"âœ… × ××¦××• {len(docs)} ×ª×•×¦××•×ª ×¡×× ×˜×™×•×ª{filter_msg}:")

            for i, doc in enumerate(docs, 1):
                score = doc.get('@search.score', 0)
                content_type_doc = doc.get('content_type', 'unknown')
                logger.info(f"\nğŸ“„ ×ª×•×¦××” {i} ({content_type_doc}, ×¦×™×•×Ÿ ×¡×× ×˜×™: {score:.3f}):")
                logger.info(f"  ğŸ†” ID: {doc.get('id', 'N/A')}")
                logger.info(f"  ğŸ“„ ××§×•×¨ ID: {doc.get('source_id', 'N/A')}")
                logger.info(f"  ğŸ“š ×§×•×¨×¡ ID: {doc.get('course_id', 'N/A')}")
                logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")
                logger.info(f"  ğŸ“… ×ª××¨×™×š ×™×¦×™×¨×”: {doc.get('created_date', 'N/A')}")

                if content_type_doc == 'video':
                    start_time = doc.get('start_time', '')
                    end_time = doc.get('end_time', '')
                    if start_time:
                        logger.info(f"  â° ×–××Ÿ: {start_time} - {end_time}")
                    keywords = doc.get('keywords', '')
                    if keywords:
                        logger.info(f"  ğŸ” ××™×œ×•×ª ××¤×ª×—: {keywords}")
                    topics = doc.get('topics', '')
                    if topics:
                        logger.info(f"  ğŸ·ï¸ × ×•×©××™×: {topics}")
                elif content_type_doc == 'document':
                    section_title = doc.get('section_title', '')
                    if section_title:
                        logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª ×¡×¢×™×£: {section_title}")

                text = doc.get('text', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")

                logger.info("â€”" * 40)

            return docs

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ×¡×× ×˜×™ ××ª×§×“×: {e}")
            logger.error(f"Error in semantic search: {e}")
            # Fallback to regular hybrid search
            return self.hybrid_search(query, top_k, source_id, course_id)

    def search_best_answers(self, query: str, k: int = 5, source_id: str = None, course_id: str = None) -> List[
        Dict]:
        """
        ×¤×•× ×§×¦×™×” ×¤×©×•×˜×” ×©××§×‘×œ×ª ×©××œ×” ×•××—×–×™×¨×” K ×”×ª×©×•×‘×•×ª ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨
        ××©×ª××©×ª ×‘×—×™×¤×•×© ×¡×× ×˜×™ ×›×‘×¨×™×¨×ª ××—×“×œ, ×¢× fallback ×œ×”×™×‘×¨×™×“×™
        ××—×–×™×¨×” ×¨×§ ××ª ×”× ×ª×•× ×™× ×œ×œ× ×”×“×¤×¡×•×ª

        Args:
            query: ×”×©××œ×” ×œ×—×™×¤×•×©
            k: ××¡×¤×¨ ×”×ª×•×¦××•×ª ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨ ×œ×”×—×–×™×¨
            source_id: ××•×¤×¦×™×•× ×œ×™ - ×× ××•×’×“×¨, ×™×—×¤×© ×¨×§ ×‘××§×•×¨ ×”×¡×¤×¦×™×¤×™ ×”×–×”
            course_id: ××•×¤×¦×™×•× ×œ×™ - ×× ××•×’×“×¨, ×™×—×¤×© ×¨×§ ×‘×§×•×¨×¡ ×”×¡×¤×¦×™×¤×™ ×”×–×”
        """
        try:
            # ×©×™××•×© ×‘×—×™×¤×•×© ×¡×× ×˜×™ ×©×”×•× ×”×›×™ ×—×›×
            results = self.semantic_search(query, k, source_id, course_id)
            return results
        except Exception:
            # fallback ×œ×”×™×‘×¨×™×“×™ ×× ×”×¡×× ×˜×™ × ×›×©×œ
            results = self.hybrid_search(query, k, source_id, course_id)
            return results

    def _build_filter_message(self, source_id: str = None, course_id: str = None) -> str:
        """×‘× ×™×™×ª ×”×•×“×¢×ª ×¤×™×œ×˜×¨ ×œ×ª×¦×•×’×”"""
        filter_parts = []
        if source_id:
            filter_parts.append(f"××§×•×¨: {source_id}")
        if course_id:
            filter_parts.append(f"×§×•×¨×¡: {course_id}")

        if filter_parts:
            return f" (××¡×•× ×Ÿ ×œ-{', '.join(filter_parts)})"
        return ""

def run_unified_search_demo():
    """×”×¨×¦×ª ×“××• ××œ× ×©×œ ××¢×¨×›×ª ×”×—×™×¤×•×© ×”×××•×—×“×ª"""
    logger.info("ğŸ” ××¢×¨×›×ª ×—×™×¤×•×© ××ª×§×“××ª ×œ×ª×•×›×Ÿ ×××•×—×“ - ×•×™×“××• ×•××¡××›×™×")
    logger.info("=" * 80)

    try:
        # ×™×¦×™×¨×ª ××¢×¨×›×ª ×”×—×™×¤×•×©
        search_system = AdvancedUnifiedContentSearch("unified-content-chunks")

        # ×‘×“×™×§×ª ××¦×‘ ×”××™× ×“×§×¡
        logger.info("\nğŸ”§ ×‘×“×™×§×ª ××¦×‘ ×”××™× ×“×§×¡ ×”×××•×—×“:")
        status = search_system.check_index_status()

        if status.get("status") != "active":
            logger.info("âŒ ×”××™× ×“×§×¡ ×œ× ×¤×¢×™×œ ××• ×¨×™×§. ×× × ×•×“× ×©×”××™× ×“×§×¡ × ×•×¦×¨ ×•××›×™×œ × ×ª×•× ×™×.")
            return

        # ×©××œ×•×ª ×œ×“×•×’××”
        demo_queries = [
            "××” ×–×” ×˜×¨× ×–×˜×™×‘×™×•×ª",
            "××ª×™ ×™×© ×©×•×•×™×•×Ÿ ×‘×™×Ÿ ××—×œ×§×•×ª ×©×§×™×œ×•×ª",
            "××™×š ××¤×©×¨ ×œ×©×œ×•×œ ×‘×™×˜×•×™"
        ]

        logger.info(f"\nğŸ¯ ×”×¨×¦×ª ×“××• ×¢× {len(demo_queries)} ×©××œ×•×ª:")

        for i, query in enumerate(demo_queries, 1):
            logger.info(f"\n{'=' * 80}")
            logger.info(f"ğŸ”¢ ×©××œ×” {i} ××ª×•×š {len(demo_queries)}: '{query}'")
            logger.info(f"{'=' * 80}")

            # 1. ×—×™×¤×•×© ×‘×›×œ ×”×ª×•×›×Ÿ
            logger.info(f"\n1ï¸âƒ£ ×—×™×¤×•×© ×‘×›×œ ×”×ª×•×›×Ÿ (×•×™×“××• + ××¡××›×™×):")
            logger.info("-" * 50)
            search_system.semantic_search(query, top_k=5)

            logger.info("\n" + "=" * 80)

            # 2. ×—×™×¤×•×© ×‘×•×™×“××• ×¡×¤×¦×™×¤×™
            logger.info(f"\n2ï¸âƒ£ ×—×™×¤×•×© ×‘×•×™×“××• ×¡×¤×¦×™×¤×™:")
            logger.info("-" * 40)
            # × × ×™×— ×©×™×© ×œ× ×• ×•×™×“××• ×¢× ID ×–×” (×ª×¦×˜×¨×š ×œ×”×—×œ×™×£ ×œID ×××™×ª×™)
            sample_video_id = "13"
            logger.info(f"ğŸ¯ ×—×™×¤×•×© ×‘×•×™×“××•: {sample_video_id}")
            search_system.semantic_search(query, top_k=5, source_id=sample_video_id)

            logger.info("\n" + "=" * 80)

            # 3. ×—×™×¤×•×© ×‘×§×•×¨×¡ ×¡×¤×¦×™×¤×™
            logger.info(f"\n3ï¸âƒ£ ×—×™×¤×•×© ×‘×§×•×¨×¡ ×¡×¤×¦×™×¤×™:")
            logger.info("-" * 35)
            sample_course_id = "Discrete_mathematics"
            logger.info(f"ğŸ¯ ×—×™×¤×•×© ×‘×§×•×¨×¡: {sample_course_id}")
            search_system.semantic_search(query, top_k=5, course_id=sample_course_id)

            # ×”×¤×¡×§×” ×‘×™×Ÿ ×©××œ×•×ª
            if i < len(demo_queries):
                logger.info("\n" + "ğŸ”„ ×¢×•×‘×¨ ×œ×©××œ×” ×”×‘××”..." + "\n")

        logger.info(f"\nğŸ‰ ×“××• ×”×•×©×œ× ×‘×”×¦×œ×—×”!")

    except Exception as e:
        logger.info(f"âŒ ×©×’×™××” ×‘×”×¨×¦×ª ×”×“××•: {e}")
        logger.error(f"Error in demo: {e}")
        traceback.print_exc()


# def main():
#     """Main function - run search demo"""
#     try:
#         run_unified_search_demo()
#     except Exception as e:
#         logger.error(f"Error in main: {e}")
#         traceback.print_exc()
#
#
# if __name__ == "__main__":
#     main()
