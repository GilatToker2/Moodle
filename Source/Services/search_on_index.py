"""
Advanced Unified Content Search - ××¢×¨×›×ª ×—×™×¤×•×© ××ª×§×“××ª ×œ×ª×•×›×Ÿ ×××•×—×“
××‘×•×¡×¡×ª ×¢×œ ×”××™× ×“×§×¡ ×”×××•×—×“ ×©× ×•×¦×¨ ×¢× unified_indexer.py
×ª×•××›×ª ×‘×—×™×¤×•×© ×‘×•×™×“××• ×•××¡××›×™× ×™×—×“ ××• ×‘× ×¤×¨×“
"""
import logging
from typing import List, Dict
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery
from openai import AzureOpenAI
import traceback
from Config.logging_config import setup_logging
logger = setup_logging()
from Config.config import (
    SEARCH_SERVICE_NAME, SEARCH_API_KEY,
    AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_VERSION,
    AZURE_OPENAI_EMBEDDING_MODEL, INDEX_NAME
)

# Configure logging - only errors and warnings
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class AdvancedUnifiedContentSearch:
    """
    ××¢×¨×›×ª ×—×™×¤×•×© ××ª×§×“××ª ×œ×ª×•×›×Ÿ ×××•×—×“ - ×•×™×“××• ×•××¡××›×™×
    ×ª×•××›×ª ×‘×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™, ×¡×× ×˜×™ ×•×•×§×˜×•×¨×™
    ×××¤×©×¨×ª ×—×™×¤×•×© ×‘×›×œ ×”×ª×•×›×Ÿ ×™×—×“ ××• ×‘×¡×™× ×•×Ÿ ×œ×¤×™ ×¡×•×’
    """

    def __init__(self, index_name: str = INDEX_NAME):
        self.index_name = INDEX_NAME
        self.search_endpoint = f"https://{SEARCH_SERVICE_NAME}.search.windows.net"
        self.credential = AzureKeyCredential(SEARCH_API_KEY)

        # ×™×¦×™×¨×ª search client
        self.search_client = SearchClient(
            endpoint=self.search_endpoint,
            index_name=self.index_name,
            credential=self.credential
        )

        # ×™×¦×™×¨×ª OpenAI client ×œ×—×™×¤×•×© ×•×§×˜×•×¨×™
        self.openai_client = AzureOpenAI(
            api_key=AZURE_OPENAI_API_KEY,
            api_version=AZURE_OPENAI_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT
        )

        logger.info(f"AdvancedUnifiedContentSearch initialized with index: {self.index_name}")

    def check_index_status(self) -> Dict:
        """×‘×“×™×§×ª ××¦×‘ ×”××™× ×“×§×¡ ×”×××•×—×“ ×•×”×¦×’×ª ××™×“×¢ ×‘×¡×™×¡×™"""
        logger.info("=" * 60)

        try:
            # ×—×™×¤×•×© ×›×œ×œ×™ ×œ×‘×“×™×§×”
            results = self.search_client.search(
                search_text="*",
                select=["*"],
                top=5,
                include_total_count=True
            )

            total_count = results.get_count()
            docs = list(results)

            logger.info(f"ğŸ“Š ×¡×”\"×› ×¦'×× ×§×™× ×‘××™× ×“×§×¡ ×”×××•×—×“: {total_count}")
            logger.info(f"ğŸ“„ ××¡××›×™× ×©×”×•×—×–×¨×• ×œ×‘×“×™×§×”: {len(docs)}")

            if docs:
                logger.info(f"âœ… ×”××™× ×“×§×¡ ×”×××•×—×“ ×¤×¢×™×œ ×•××›×™×œ × ×ª×•× ×™×")

                # ×¡×¤×™×¨×” ×œ×¤×™ ×¡×•×’ ×ª×•×›×Ÿ
                video_results = self.search_client.search("*", filter="content_type eq 'video'",
                                                          include_total_count=True, top=0)
                video_count = video_results.get_count()

                doc_results = self.search_client.search("*", filter="content_type eq 'document'",
                                                        include_total_count=True, top=0)
                doc_count = doc_results.get_count()

                logger.info(f"ğŸ¥ ×•×™×“××• ×¦'×× ×§×™×: {video_count}")
                logger.info(f"ğŸ“ ××¡××š ×¦'×× ×§×™×: {doc_count}")

                # ×”×¦×’×ª ×“×•×’×××•×ª ×œ××¡××›×™×
                logger.info(f"\nğŸ“„ ×“×•×’×××•×ª ×œ××¡××›×™× ×‘××™× ×“×§×¡:")
                for i, doc in enumerate(docs[:10], 1):
                    content_type = doc.get('content_type', 'unknown')
                    logger.info(f"\nğŸ“„ ××¡××š {i} ({content_type}):")
                    logger.info(f"  ğŸ†” ID: {doc.get('id', 'N/A')}")
                    logger.info(f"  ğŸ“„ Source ID: {doc.get('source_id', 'N/A')}")
                    logger.info(f"  ğŸ“ Source Name: {doc.get('source_name', 'N/A')}")
                    logger.info(f"  ğŸ“‘ Chunk Index: {doc.get('chunk_index', 'N/A')}")

                    if content_type == 'video':
                        logger.info(f"  â° Start Time: {doc.get('start_time', 'N/A')}")
                        logger.info(f"  â±ï¸ Start Seconds: {doc.get('start_seconds', 'N/A')}")
                    elif content_type == 'document':
                        logger.info(f"  ğŸ“‹ Section Title: {doc.get('section_title', 'N/A')}")
                        logger.info(f"  ğŸ“„ Document Type: {doc.get('document_type', 'N/A')}")

                    # ×”×¦×’×ª ×ª×•×›×Ÿ ×”×˜×§×¡×˜
                    text = doc.get('text', '')
                    if text:
                        preview = text[:150] + "..." if len(text) > 150 else text
                        logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")
                    logger.info("-" * 30)

                return {
                    "status": "active",
                    "total_chunks": total_count,
                    "video_chunks": video_count,
                    "document_chunks": doc_count,
                    "sample_doc": docs[0] if docs else None
                }
            else:
                logger.info("âš ï¸ ×”××™× ×“×§×¡ ×§×™×™× ××‘×œ ×¨×™×§")
                return {"status": "empty", "total_chunks": 0}

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×’×™×©×” ×œ××™× ×“×§×¡: {e}")
            logger.error(f"Error checking index status: {e}")
            return {"status": "error", "error": str(e)}

    def generate_query_embedding(self, query: str) -> List[float]:
        """×™×¦×™×¨×ª embedding ×œ×©××œ×ª ×”×—×™×¤×•×©"""
        try:
            response = self.openai_client.embeddings.create(
                model=AZURE_OPENAI_EMBEDDING_MODEL,
                input=query
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating query embedding: {e}")
            return []

    def simple_text_search(self, query: str, top_k: int = 5, content_type: str = None, source_id: str = None) -> List[
        Dict]:
        """×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×¤×©×•×˜ ×‘××™×“×” ×•×œ× × ×™×ª×Ÿ ×œ×—×œ×¥ embedding"""
        logger.info("=" * 60)

        try:
            search_params = {
                "search_text": query,
                "select": [
                    "id", "content_type", "source_id", "chunk_index",
                    "text", "start_time", "end_time", "section_title", "created_date", "keywords", "topics"
                ],
                "top": top_k,
                "include_total_count": True
            }

            # ×”×•×¡×¤×ª ×¤×™×œ×˜×¨×™×
            filters = []
            if content_type:
                filters.append(f"content_type eq '{content_type}'")
            if source_id:
                escaped_source_id = source_id.replace("'", "''")
                filters.append(f"source_id eq '{escaped_source_id}'")

            if filters:
                search_params["filter"] = " and ".join(filters)

            results = self.search_client.search(**search_params)

            docs = list(results)
            total_count = results.get_count()

            if not docs:
                logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª")
                return []

            filter_msg = self._build_filter_message(content_type, source_id)
            logger.info(f"âœ… × ××¦××• {len(docs)} ×ª×•×¦××•×ª ××ª×•×š {total_count} ×¦'×× ×§×™×{filter_msg}:")

            for i, doc in enumerate(docs, 1):
                score = doc.get('@search.score', 0)
                content_type_doc = doc.get('content_type', 'unknown')
                logger.info(f"\nğŸ“„ ×ª×•×¦××” {i} ({content_type_doc}, ×¦×™×•×Ÿ: {score:.3f}):")
                logger.info(f"  ğŸ“„ ××§×•×¨ ID: {doc.get('source_id', 'N/A')}")
                logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")

                if content_type_doc == 'video':
                    start_time = doc.get('start_time', '')
                    end_time = doc.get('end_time', '')
                    if start_time:
                        logger.info(f"  â° ×–××Ÿ: {start_time} - {end_time}")
                elif content_type_doc == 'document':
                    section_title = doc.get('section_title', '')
                    if section_title:
                        logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª: {section_title}")

                text = doc.get('text', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")

                logger.info("â€”" * 40)

            return docs

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™: {e}")
            logger.error(f"Error in text search: {e}")
            return []

    def hybrid_search(self, query: str, top_k: int = 5, content_type: str = None, source_id: str = None) -> List[Dict]:
        """×—×™×¤×•×© ×”×™×‘×¨×™×“×™ - ××©×œ×‘ ×˜×§×¡×˜ ×•×•×§×˜×•×¨"""
        logger.info("=" * 60)

        try:
            # ×™×¦×™×¨×ª embedding ×œ×©××œ×”
            query_vector = self.generate_query_embedding(query)
            if not query_vector:
                logger.info("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×™×¦×•×¨ embedding, ××‘×¦×¢ ×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×‘×œ×‘×“")
                return self.simple_text_search(query, top_k, content_type, source_id)

            search_params = {
                "search_text": query,
                "vector_queries": [VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=50,
                    fields="vector"
                )],
                "select": [
                    "id", "content_type", "source_id", "chunk_index",
                    "text", "start_time", "end_time", "section_title", "created_date", "keywords", "topics"
                ],
                "top": 50,
                "include_total_count": True
            }

            # ×”×•×¡×¤×ª ×¤×™×œ×˜×¨×™×
            filters = []
            if content_type:
                filters.append(f"content_type eq '{content_type}'")
            if source_id:
                escaped_source_id = source_id.replace("'", "''")
                filters.append(f"source_id eq '{escaped_source_id}'")

            if filters:
                search_params["filter"] = " and ".join(filters)

            results = self.search_client.search(**search_params)

            docs = list(results)
            total_count = results.get_count()

            if not docs:
                logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª ×”×™×‘×¨×™×“×™×•×ª")
                return []

            # Slice to requested top_k for display and return
            docs = docs[:top_k]

            filter_msg = self._build_filter_message(content_type, source_id)
            logger.info(f"âœ… × ××¦××• {len(docs)} ×ª×•×¦××•×ª ×”×™×‘×¨×™×“×™×•×ª ××ª×•×š {total_count} ×¦'×× ×§×™×{filter_msg}:")

            for i, doc in enumerate(docs, 1):
                score = doc.get('@search.score', 0)
                content_type_doc = doc.get('content_type', 'unknown')
                logger.info(f"\nğŸ“„ ×ª×•×¦××” {i} ({content_type_doc}, ×¦×™×•×Ÿ ××©×•×œ×‘: {score:.3f}):")
                logger.info(f"  ğŸ“„ ××§×•×¨ ID: {doc.get('source_id', 'N/A')}")
                logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")

                if content_type_doc == 'video':
                    start_time = doc.get('start_time', '')
                    end_time = doc.get('end_time', '')
                    if start_time:
                        logger.info(f"  â° ×–××Ÿ: {start_time} - {end_time}")
                elif content_type_doc == 'document':
                    section_title = doc.get('section_title', '')
                    if section_title:
                        logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª: {section_title}")

                text = doc.get('text', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")

                logger.info("â€”" * 40)

            return docs

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ×”×™×‘×¨×™×“×™: {e}")
            logger.error(f"Error in hybrid search: {e}")
            return []

    def semantic_search(self, query: str, top_k: int = 5, content_type: str = None, source_id: str = None) -> List[
        Dict]:
        """×—×™×¤×•×© ×¡×× ×˜×™ ××ª×§×“×"""
        logger.info("=" * 60)

        try:
            # ×™×¦×™×¨×ª embedding ×œ×©××œ×”
            query_vector = self.generate_query_embedding(query)
            if not query_vector:
                logger.info("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×™×¦×•×¨ embedding, ××‘×¦×¢ ×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×‘×œ×‘×“")
                return self.simple_text_search(query, top_k, content_type, source_id)

            # ×”×›× ×ª ×¤×¨××˜×¨×™× ×œ×—×™×¤×•×©
            search_params = {
                "search_text": query,
                "query_type": "semantic",
                "semantic_configuration_name": "default",
                "query_language": "he-il",
                "highlight_fields": "text",
                "vector_queries": [VectorizedQuery(
                    vector=query_vector,
                    k_nearest_neighbors=top_k,
                    fields="vector"
                )],
                "select": [
                    "id", "content_type", "source_id", "chunk_index",
                    "text", "start_time", "end_time", "section_title", "created_date", "keywords", "topics"
                ],
                "top": top_k
            }

            # ×”×•×¡×¤×ª ×¤×™×œ×˜×¨×™×
            filters = []
            if content_type:
                filters.append(f"content_type eq '{content_type}'")
            if source_id:
                escaped_source_id = source_id.replace("'", "''")
                filters.append(f"source_id eq '{escaped_source_id}'")

            if filters:
                search_params["filter"] = " and ".join(filters)

            # ×—×™×¤×•×© ×¡×× ×˜×™ ××ª×§×“×
            results = self.search_client.search(**search_params)

            docs = list(results)

            if not docs:
                logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª ×¡×× ×˜×™×•×ª")
                return []

            filter_msg = self._build_filter_message(content_type, source_id)
            logger.info(f"âœ… × ××¦××• {len(docs)} ×ª×•×¦××•×ª ×¡×× ×˜×™×•×ª{filter_msg}:")

            for i, doc in enumerate(docs, 1):
                score = doc.get('@search.score', 0)
                content_type_doc = doc.get('content_type', 'unknown')
                logger.info(f"\nğŸ“„ ×ª×•×¦××” {i} ({content_type_doc}, ×¦×™×•×Ÿ ×¡×× ×˜×™: {score:.3f}):")
                logger.info(f"  ğŸ“„ ××§×•×¨ ID: {doc.get('source_id', 'N/A')}")
                logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")

                if content_type_doc == 'video':
                    start_time = doc.get('start_time', '')
                    end_time = doc.get('end_time', '')
                    if start_time:
                        logger.info(f"  â° ×–××Ÿ: {start_time} - {end_time}")
                elif content_type_doc == 'document':
                    section_title = doc.get('section_title', '')
                    if section_title:
                        logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª: {section_title}")

                text = doc.get('text', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    logger.info(f"  ğŸ“œ ×ª×•×›×Ÿ: {preview}")

                logger.info("â€”" * 40)

            return docs

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×© ×¡×× ×˜×™ ××ª×§×“×: {e}")
            logger.error(f"Error in semantic search: {e}")
            # Fallback to regular hybrid search
            return self.hybrid_search(query, top_k, content_type, source_id)

    def search_best_answers(self, query: str, k: int = 5, content_type: str = None, source_id: str = None) -> List[
        Dict]:
        """
        ×¤×•× ×§×¦×™×” ×¤×©×•×˜×” ×©××§×‘×œ×ª ×©××œ×” ×•××—×–×™×¨×” K ×”×ª×©×•×‘×•×ª ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨
        ××©×ª××©×ª ×‘×—×™×¤×•×© ×¡×× ×˜×™ ×›×‘×¨×™×¨×ª ××—×“×œ, ×¢× fallback ×œ×”×™×‘×¨×™×“×™
        ××—×–×™×¨×” ×¨×§ ××ª ×”× ×ª×•× ×™× ×œ×œ× ×”×“×¤×¡×•×ª

        Args:
            query: ×”×©××œ×” ×œ×—×™×¤×•×©
            k: ××¡×¤×¨ ×”×ª×•×¦××•×ª ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨ ×œ×”×—×–×™×¨
            content_type: ××•×¤×¦×™×•× ×œ×™ - "video" ××• "document" ×œ×¡×™× ×•×Ÿ ×œ×¤×™ ×¡×•×’ ×ª×•×›×Ÿ
            source_id: ××•×¤×¦×™×•× ×œ×™ - ×× ××•×’×“×¨, ×™×—×¤×© ×¨×§ ×‘××§×•×¨ ×”×¡×¤×¦×™×¤×™ ×”×–×”
        """
        try:
            # ×©×™××•×© ×‘×—×™×¤×•×© ×¡×× ×˜×™ ×©×”×•× ×”×›×™ ×—×›×
            results = self.semantic_search(query, k, content_type, source_id)
            return results
        except Exception:
            # fallback ×œ×”×™×‘×¨×™×“×™ ×× ×”×¡×× ×˜×™ × ×›×©×œ
            results = self.hybrid_search(query, k, content_type, source_id)
            return results

    def search_videos_only(self, query: str, k: int = 5, video_id: str = None) -> List[Dict]:
        """×—×™×¤×•×© ×¨×§ ×‘×•×™×“××• - ×ª××™××•×ª ×œ××—×•×¨ ×¢× ×”×§×•×‘×¥ ×”××§×•×¨×™"""
        return self.search_best_answers(query, k, content_type="video", source_id=video_id)

    def search_documents_only(self, query: str, k: int = 5, document_id: str = None) -> List[Dict]:
        """×—×™×¤×•×© ×¨×§ ×‘××¡××›×™×"""
        return self.search_best_answers(query, k, content_type="document", source_id=document_id)

    def get_video_transcript(self, video_id: str) -> Dict:
        """×§×‘×œ×ª ×ª××œ×•×œ ××œ× ×©×œ ×”×¨×¦××” ×¡×¤×¦×™×¤×™×ª ×œ×¤×™ ID - ×›××• ×‘×§×•×‘×¥ ×”××§×•×¨×™"""
        logger.info(f"ğŸ“¹ ×§×‘×œ×ª ×ª××œ×•×œ ××œ× ×¢×‘×•×¨ ×•×™×“××• ID: {video_id}")
        logger.info("=" * 60)

        try:
            # ×§×‘×œ×ª ×›×œ ×”×¦'×× ×§×™× ×©×œ ×”×•×™×“××•
            results = self.search_client.search(
                search_text="*",
                filter=f"source_id eq '{video_id}' and content_type eq 'video'",
                select=["start_time", "end_time", "text", "created_date", "chunk_index"],
                order_by=["chunk_index asc"],
                top=1000  # ××¡×¤×¨ ×’×‘×•×” ×œ×§×‘×œ×ª ×›×œ ×”×¦'×× ×§×™×
            )

            docs = list(results)

            if not docs:
                logger.info(f"âŒ ×œ× × ××¦× ×•×™×“××• ×¢× ID: {video_id}")
                return {"error": f"×œ× × ××¦× ×•×™×“××• ×¢× ID: {video_id}"}

            logger.info(f"ğŸ¬ ×•×™×“××• ID: {video_id}")
            logger.info(f"ğŸ“„ ×¡×”\"×› ×¦'×× ×§×™×: {len(docs)}")
            logger.info("\nğŸ“œ ×ª××œ×•×œ ××œ×:")
            logger.info("=" * 60)

            # ×”×“×¤×¡×ª ×”×ª××œ×•×œ ×”××œ×
            for i, doc in enumerate(docs, 1):
                start_time = doc.get('start_time', 'N/A')
                end_time = doc.get('end_time', 'N/A')
                text = doc.get('text', '')

                logger.info(f"\nâ° [{start_time} - {end_time}]")
                logger.info(f"{text}")
                logger.info("-" * 40)

            # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª
            total_chunks = len(docs)
            total_text_length = sum(len(doc.get('text', '')) for doc in docs)

            summary = {
                "video_id": video_id,
                "total_chunks": total_chunks,
                "total_text_characters": total_text_length,
                "average_chunk_length": round(total_text_length / total_chunks) if total_chunks > 0 else 0,
                "created_date": docs[0].get('created_date') if docs else None,
                "transcript": docs
            }

            logger.info(f"\nğŸ“Š ×¡×™×›×•×:")
            logger.info(f"  ğŸ“ ×•×™×“××• ID: {summary['video_id']}")
            logger.info(f"  ğŸ“„ ×¦'×× ×§×™×: {summary['total_chunks']}")
            logger.info(f"  ğŸ“ ××•×¨×š ×˜×§×¡×˜ ×›×•×œ×œ: {summary['total_text_characters']} ×ª×•×•×™×")

            return summary

        except Exception as e:
            logger.info(f"âŒ ×©×’×™××” ×‘×§×‘×œ×ª ×ª××œ×•×œ: {e}")
            logger.error(f"Error getting video transcript: {e}")
            return {"error": str(e)}

    def _build_filter_message(self, content_type: str = None, source_id: str = None) -> str:
        """×‘× ×™×™×ª ×”×•×“×¢×ª ×¤×™×œ×˜×¨ ×œ×ª×¦×•×’×”"""
        filter_parts = []
        if content_type:
            filter_parts.append(f"×¡×•×’: {content_type}")
        if source_id:
            filter_parts.append(f"××§×•×¨: {source_id}")

        if filter_parts:
            return f" (××¡×•× ×Ÿ ×œ-{', '.join(filter_parts)})"
        return ""

    def get_content_summary(self, source_id: str, content_type: str = None) -> Dict:
        """×§×‘×œ×ª ×¡×™×›×•× ×©×œ ××§×•×¨ ×¡×¤×¦×™×¤×™ (×•×™×“××• ××• ××¡××š)"""
        try:
            # ×‘× ×™×™×ª ×¤×™×œ×˜×¨
            filters = [f"source_id eq '{source_id}'"]
            if content_type:
                filters.append(f"content_type eq '{content_type}'")

            filter_str = " and ".join(filters)

            # ×§×‘×œ×ª ×›×œ ×”×¦'×× ×§×™× ×©×œ ×”××§×•×¨
            results = self.search_client.search(
                search_text="*",
                filter=filter_str,
                select=["content_type", "text", "section_title", "created_date", "chunk_index"],
                order_by=["chunk_index asc"],
                top=1000  # ××¡×¤×¨ ×’×‘×•×” ×œ×§×‘×œ×ª ×›×œ ×”×¦'×× ×§×™×
            )


            docs = list(results)

            if not docs:
                return {"error": f"×œ× × ××¦× ×ª×•×›×Ÿ ×¢× ID: {source_id}"}

            # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª
            total_chunks = len(docs)
            detected_content_type = docs[0].get('content_type', 'unknown')
            total_text_length = sum(len(doc.get('text', '')) for doc in docs)

            summary = {
                "source_id": source_id,
                "content_type": detected_content_type,
                "total_chunks": total_chunks,
                "total_text_characters": total_text_length,
                "average_chunk_length": round(total_text_length / total_chunks) if total_chunks > 0 else 0,
                "created_date": docs[0].get('created_date') if docs else None
            }

            # ×”×•×¡×¤×ª ××™×“×¢ ×¡×¤×¦×™×¤×™ ×œ×¡×•×’ ×”×ª×•×›×Ÿ
            if detected_content_type == 'document':
                # ×¡×¤×™×¨×ª ×¡×¢×™×¤×™×
                sections = set(doc.get('section_title', '') for doc in docs if doc.get('section_title'))
                summary["unique_sections"] = len(sections)
                summary["section_titles"] = list(sections)

            logger.info(f"ğŸ“Š ×¡×™×›×•× {detected_content_type} {source_id}:")
            logger.info(f"  ğŸ“ ××§×•×¨ ID: {source_id}")
            logger.info(f"  ğŸ“„ ×¦'×× ×§×™×: {summary['total_chunks']}")
            logger.info(f"  ğŸ“ ××•×¨×š ×˜×§×¡×˜ ×›×•×œ×œ: {summary['total_text_characters']} ×ª×•×•×™×")
            logger.info(f"  ğŸ“Š ××•×¨×š ×¦'×× ×§ ×××•×¦×¢: {summary['average_chunk_length']} ×ª×•×•×™×")

            if detected_content_type == 'document' and 'unique_sections' in summary:
                logger.info(f"  ğŸ“‹ ×¡×¢×™×¤×™×: {summary['unique_sections']}")

            return summary

        except Exception as e:
            logger.error(f"Error creating content summary: {e}")
            return {"error": str(e)}


def run_unified_search_demo():
    """×”×¨×¦×ª ×“××• ××œ× ×©×œ ××¢×¨×›×ª ×”×—×™×¤×•×© ×”×××•×—×“×ª"""
    logger.info("ğŸ” ××¢×¨×›×ª ×—×™×¤×•×© ××ª×§×“××ª ×œ×ª×•×›×Ÿ ×××•×—×“ - ×•×™×“××• ×•××¡××›×™×")
    logger.info("=" * 80)

    try:
        # ×™×¦×™×¨×ª ××¢×¨×›×ª ×”×—×™×¤×•×©
        search_system = AdvancedUnifiedContentSearch("unified-content-chunks")

        # ×‘×“×™×§×ª ××¦×‘ ×”××™× ×“×§×¡
        logger.info("\nğŸ”§ ×‘×“×™×§×ª ××¦×‘ ×”××™× ×“×§×¡ ×”×××•×—×“:")
        status = search_system.check_index_status()

        if status.get("status") != "active":
            logger.info("âŒ ×”××™× ×“×§×¡ ×œ× ×¤×¢×™×œ ××• ×¨×™×§. ×× × ×•×“× ×©×”××™× ×“×§×¡ × ×•×¦×¨ ×•××›×™×œ × ×ª×•× ×™×.")
            return

        # ×©××œ×•×ª ×œ×“×•×’××”
        demo_queries = [
            "××” ×–×” ×˜×¨× ×–×˜×™×‘×™×•×ª",
            "××ª×™ ×™×© ×©×•×•×™×•×Ÿ ×‘×™×Ÿ ××—×œ×§×•×ª ×©×§×™×œ×•×ª",
            "××™×š ××¤×©×¨ ×œ×©×œ×•×œ ×‘×™×˜×•×™"
        ]

        logger.info(f"\nğŸ¯ ×”×¨×¦×ª ×“××• ×¢× {len(demo_queries)} ×©××œ×•×ª:")

        for i, query in enumerate(demo_queries, 1):
            logger.info(f"\n{'=' * 80}")
            logger.info(f"ğŸ”¢ ×©××œ×” {i} ××ª×•×š {len(demo_queries)}: '{query}'")
            logger.info(f"{'=' * 80}")

            # 1. ×—×™×¤×•×© ×‘×›×œ ×”×ª×•×›×Ÿ
            logger.info(f"\n1ï¸âƒ£ ×—×™×¤×•×© ×‘×›×œ ×”×ª×•×›×Ÿ (×•×™×“××• + ××¡××›×™×):")
            logger.info("-" * 50)
            search_system.semantic_search(query, top_k=3)

            logger.info("\n" + "=" * 80)

            # 2. ×—×™×¤×•×© ×¨×§ ×‘×•×™×“××•
            logger.info(f"\n2ï¸âƒ£ ×—×™×¤×•×© ×¨×§ ×‘×•×™×“××•:")
            logger.info("-" * 30)
            search_system.semantic_search(query, top_k=2, content_type="video")

            logger.info("\n" + "=" * 80)

            # 3. ×—×™×¤×•×© ×¨×§ ×‘××¡××›×™×
            logger.info(f"\n3ï¸âƒ£ ×—×™×¤×•×© ×¨×§ ×‘××¡××›×™×:")
            logger.info("-" * 35)
            search_system.semantic_search(query, top_k=2, content_type="document")

            logger.info("\n" + "=" * 80)

            # 4. ×—×™×¤×•×© ×‘×•×™×“××• ×¡×¤×¦×™×¤×™ - ×›××• ×‘×§×•×‘×¥ ×”××§×•×¨×™
            logger.info(f"\n4ï¸âƒ£ ×—×™×¤×•×© ×‘×•×™×“××• ×¡×¤×¦×™×¤×™:")
            logger.info("-" * 40)
            # × × ×™×— ×©×™×© ×œ× ×• ×•×™×“××• ×¢× ID ×–×” (×ª×¦×˜×¨×š ×œ×”×—×œ×™×£ ×œID ×××™×ª×™)
            sample_video_id = "2"
            logger.info(f"ğŸ¯ ×—×™×¤×•×© ×‘×•×™×“××•: {sample_video_id}")
            search_system.semantic_search(query, top_k=2, content_type="video", source_id=sample_video_id)

            # ×”×¤×¡×§×” ×‘×™×Ÿ ×©××œ×•×ª
            if i < len(demo_queries):
                logger.info("\n" + "ğŸ”„ ×¢×•×‘×¨ ×œ×©××œ×” ×”×‘××”..." + "\n")

        # ×¡×™×›×•× ×ª×•×›×Ÿ ×œ×“×•×’××”
        if status.get("sample_doc"):
            sample_source_id = status["sample_doc"].get("source_id")
            sample_content_type = status["sample_doc"].get("content_type")
            if sample_source_id:
                logger.info(f"\n{'=' * 80}")
                logger.info(f"ğŸ“Š ×¡×™×›×•× ×”×ª×•×›×Ÿ ×œ×“×•×’××”")
                logger.info(f"{'=' * 80}")
                search_system.get_content_summary(sample_source_id, sample_content_type)

        logger.info(f"\nğŸ‰ ×“××• ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
        logger.info("ğŸ’¡ × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘××¢×¨×›×ª ×¢× ×©××œ×•×ª × ×•×¡×¤×•×ª ××• ×œ×—×¤×© ×œ×¤×™ ×¡×•×’ ×ª×•×›×Ÿ ×¡×¤×¦×™×¤×™")

    except Exception as e:
        logger.info(f"âŒ ×©×’×™××” ×‘×”×¨×¦×ª ×”×“××•: {e}")
        logger.error(f"Error in demo: {e}")
        traceback.logger.info_exc()


def main():
    """×¤×•× ×§×¦×™×” ×¨××©×™×ª"""
    try:
        # ×”×¨×¦×ª ×“××•
        run_unified_search_demo()

        # ×™×¦×™×¨×ª ××¢×¨×›×ª ×”×—×™×¤×•×©
        search_system = AdvancedUnifiedContentSearch("unified-content-chunks")

        # ×©××œ×” ×œ×“×•×’××”
        query = "××” ×”×”×’×“×¨×” ×©×œ ×™×—×¡ ×©×§×™×œ×•×ª"

        logger.info(f"\nğŸ” ×—×™×¤×•×© ×××•×—×“ ×¢×‘×•×¨: '{query}'")
        logger.info("=" * 60)

        # ×—×™×¤×•×© ×¤×©×•×˜ ××—×“ ×‘×œ×‘×“
        results = search_system.search_best_answers(query, k=5)

        if not results:
            logger.info("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª")
            return

        logger.info(f"\nğŸ“‹ {len(results)} ×”×ª×©×•×‘×•×ª ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨:")
        logger.info("=" * 60)

        for i, doc in enumerate(results, 1):
            score = doc.get('@search.score', 0)
            content_type = doc.get('content_type', 'unknown')
            logger.info(f"\nğŸ† ×ª×©×•×‘×” {i} ({content_type}, ×¦×™×•×Ÿ: {score:.3f}):")
            logger.info(f"  ğŸ“„ ××§×•×¨: {doc.get('source_name', '×œ× ×–××™×Ÿ')}")
            logger.info(f"  ğŸ“‘ ×¦'×× ×§: {doc.get('chunk_index', 'N/A')}")

            if content_type == 'video':
                start_time = doc.get('start_time', '')
                if start_time:
                    logger.info(f"  â° ×–××Ÿ: {start_time}")
            elif content_type == 'document':
                section_title = doc.get('section_title', '')
                if section_title:
                    logger.info(f"  ğŸ“‹ ×›×•×ª×¨×ª: {section_title}")

            text = doc.get('text', '')
            if text:
                logger.info(f"  ğŸ’¬ ×ª×•×›×Ÿ: {text}")

            logger.info("-" * 40)

    except Exception as e:
        logger.info(f"âŒ ×©×’×™××” ×›×œ×œ×™×ª: {e}")
        traceback.logger.info_exc()


if __name__ == "__main__":
    main()
